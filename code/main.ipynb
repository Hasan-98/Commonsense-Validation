{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import ChartParser\n",
    "from nltk.grammar import CFG\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='input file name')\n",
    "# parser.add_argument('--input',required=True, help=\"Please add the input file name path and name\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Read Data from input file given by args.\n",
    "# df=pd.read_csv(args.input, sep='\\t',index_col='id')"
   ]
  },
  {
   "source": [
    "# Task A"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../ALL data/Training  Data/subtaskA_data_all.csv', sep=',',index_col='id')\n",
    "df_label=pd.read_csv('../ALL data/Training  Data/subtaskA_answers_all.csv', sep=',',header=None,index_col=0)\n",
    "df_label.columns=['label-false']\n",
    "df=df.join(df_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  sent0  \\\n",
       "id                                                        \n",
       "0                 He poured orange juice on his cereal.   \n",
       "1                                      He drinks apple.   \n",
       "2                                 Jeff ran a mile today   \n",
       "3                                  A mosquito stings me   \n",
       "4                                  A niece is a person.   \n",
       "...                                                 ...   \n",
       "9995                   Mark ate a big bitter cherry pie   \n",
       "9996                     Gloria wears a cat on her head   \n",
       "9997  Harry went to the barbershop to have his hair cut   \n",
       "9998                    Reilly is sleeping on the couch   \n",
       "9999                           I have a desk on my lamp   \n",
       "\n",
       "                                                  sent1  label-false  \n",
       "id                                                                    \n",
       "0                         He poured milk on his cereal.            0  \n",
       "1                                       He drinks milk.            0  \n",
       "2                          Jeff ran 100,000 miles today            1  \n",
       "3                                    I sting a mosquito            1  \n",
       "4                                A giraffe is a person.            1  \n",
       "...                                                 ...          ...  \n",
       "9995                    Mark ate a big sweet cherry pie            0  \n",
       "9996                     Gloria wears a hat on her head            0  \n",
       "9997  Harry went to the barbershop to have his glass...            1  \n",
       "9998                   Reilly is sleeping on the window            1  \n",
       "9999                           I have a lamp on my desk            0  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent0</th>\n      <th>sent1</th>\n      <th>label-false</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>He poured orange juice on his cereal.</td>\n      <td>He poured milk on his cereal.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>He drinks apple.</td>\n      <td>He drinks milk.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jeff ran a mile today</td>\n      <td>Jeff ran 100,000 miles today</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A mosquito stings me</td>\n      <td>I sting a mosquito</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A niece is a person.</td>\n      <td>A giraffe is a person.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>Mark ate a big bitter cherry pie</td>\n      <td>Mark ate a big sweet cherry pie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>Gloria wears a cat on her head</td>\n      <td>Gloria wears a hat on her head</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>Harry went to the barbershop to have his hair cut</td>\n      <td>Harry went to the barbershop to have his glass...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>Reilly is sleeping on the couch</td>\n      <td>Reilly is sleeping on the window</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>I have a desk on my lamp</td>\n      <td>I have a lamp on my desk</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.39989597379060515\n",
      "-------------\n",
      "0.42548061000793774\n",
      "-------------\n",
      "0.5156204540425442\n",
      "-------------\n",
      "0.4335789488698799\n",
      "-------------\n",
      "0.5957062266924752\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for index, row in df1.iterrows():  \n",
    "# tokenize the rows of dataframe (pos column)\n",
    "    sentence_1 = row['sent0']\n",
    "    sentence_2 = row['sent1']\n",
    "\n",
    "    prob_1 = predict(sentence_1, bert_model=model, bert_tokenizer=tokenizer)\n",
    "    prob_2 = predict(sentence_2, bert_model=model, bert_tokenizer=tokenizer) \n",
    "    df1.loc[index,'prob_1']=prob_1\n",
    "    df1.loc[index,'prob_2']=prob_2\n",
    "    df1.loc[index,'predicted_false']=np.argmin([prob_1, prob_2]).astype('int64')\n",
    "\n",
    "    print(np.min([prob_1, prob_2]))\n",
    "    print('-------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                    sent0                          sent1  \\\n",
       "id                                                                         \n",
       "0   He poured orange juice on his cereal.  He poured milk on his cereal.   \n",
       "1                        He drinks apple.                He drinks milk.   \n",
       "2                   Jeff ran a mile today   Jeff ran 100,000 miles today   \n",
       "3                    A mosquito stings me             I sting a mosquito   \n",
       "4                    A niece is a person.         A giraffe is a person.   \n",
       "\n",
       "    label-false    prob_1    prob_2  predicted_false  \n",
       "id                                                    \n",
       "0             0  0.399896  0.768483              0.0  \n",
       "1             0  0.610041  0.425481              1.0  \n",
       "2             1  0.603129  0.515620              1.0  \n",
       "3             1  0.458191  0.433579              1.0  \n",
       "4             1  0.715048  0.595706              1.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent0</th>\n      <th>sent1</th>\n      <th>label-false</th>\n      <th>prob_1</th>\n      <th>prob_2</th>\n      <th>predicted_false</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>He poured orange juice on his cereal.</td>\n      <td>He poured milk on his cereal.</td>\n      <td>0</td>\n      <td>0.399896</td>\n      <td>0.768483</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>He drinks apple.</td>\n      <td>He drinks milk.</td>\n      <td>0</td>\n      <td>0.610041</td>\n      <td>0.425481</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jeff ran a mile today</td>\n      <td>Jeff ran 100,000 miles today</td>\n      <td>1</td>\n      <td>0.603129</td>\n      <td>0.515620</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A mosquito stings me</td>\n      <td>I sting a mosquito</td>\n      <td>1</td>\n      <td>0.458191</td>\n      <td>0.433579</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A niece is a person.</td>\n      <td>A giraffe is a person.</td>\n      <td>1</td>\n      <td>0.715048</td>\n      <td>0.595706</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy is equal to: 80.0 %\n"
     ]
    }
   ],
   "source": [
    "# print accuracy report\n",
    "print(f\"Accuracy is equal to: {((df1['label-false']==df1['predicted_false']).sum()/df1.shape[0])*100} %\")"
   ]
  },
  {
   "source": [
    "# Task B"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../ALL data/Training  Data/subtaskB_data_all.csv', sep=',',index_col='id')\n",
    "df_label=pd.read_csv('../ALL data/Training  Data/subtaskB_answers_all.csv', sep=',',header=None,index_col=0)\n",
    "df_label.columns=['label']\n",
    "df=df.join(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              FalseSent  \\\n",
       "id                                                        \n",
       "0                 He poured orange juice on his cereal.   \n",
       "1                                      He drinks apple.   \n",
       "2                          Jeff ran 100,000 miles today   \n",
       "3                                    I sting a mosquito   \n",
       "4                                A giraffe is a person.   \n",
       "...                                                 ...   \n",
       "9995                   Mark ate a big bitter cherry pie   \n",
       "9996                     Gloria wears a cat on her head   \n",
       "9997  Harry went to the barbershop to have his glass...   \n",
       "9998                   Reilly is sleeping on the window   \n",
       "9999                           I have a desk on my lamp   \n",
       "\n",
       "                                                OptionA  \\\n",
       "id                                                        \n",
       "0                Orange juice is usually bright orange.   \n",
       "1               Apple juice are very tasty and milk too   \n",
       "2     100,000 miles is way to long for one person to...   \n",
       "3                                   A human is a mammal   \n",
       "4                 Giraffes can drink water from a lake.   \n",
       "...                                                 ...   \n",
       "9995                   Mark is bad at making cherry pie   \n",
       "9996               a hat cannot be worn on a cat's head   \n",
       "9997  a barbershop usually don't provide the service...   \n",
       "9998   the window is open and a person cannot lay on it   \n",
       "9999          the lamp is made of glass that is fragile   \n",
       "\n",
       "                                                OptionB  \\\n",
       "id                                                        \n",
       "0            Orange juice doesn't taste good on cereal.   \n",
       "1                                Apple can not be drunk   \n",
       "2     Jeff is a four letter name and 100,000 has six...   \n",
       "3                                 A human is omnivorous   \n",
       "4                       A giraffe is not a human being.   \n",
       "...                                                 ...   \n",
       "9995                         a cherry pie should be big   \n",
       "9996            a cat cannot be worn on a person's head   \n",
       "9997  a barbershop usually repairs computers instead...   \n",
       "9998              the window is too cold to sleep on it   \n",
       "9999                        the lamp is of poor quality   \n",
       "\n",
       "                                                OptionC label  \n",
       "id                                                             \n",
       "0     Orange juice is sticky if you spill it on the ...     B  \n",
       "1                              Apple cannot eat a human     B  \n",
       "2              100,000 miles is longer than 100,000 km.     A  \n",
       "3                                A human has not stings     C  \n",
       "4                         .Giraffes usually eat leaves.     B  \n",
       "...                                                 ...   ...  \n",
       "9995                       a cherry pie should be sweet     C  \n",
       "9996        the cat is too heavy to be worn on her head     B  \n",
       "9997  the barbershop lacked the necessary tools to r...     A  \n",
       "9998                  a person cannot sleep on a window     C  \n",
       "9999              a desk is too big to be put on a lamp     C  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FalseSent</th>\n      <th>OptionA</th>\n      <th>OptionB</th>\n      <th>OptionC</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>He poured orange juice on his cereal.</td>\n      <td>Orange juice is usually bright orange.</td>\n      <td>Orange juice doesn't taste good on cereal.</td>\n      <td>Orange juice is sticky if you spill it on the ...</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>He drinks apple.</td>\n      <td>Apple juice are very tasty and milk too</td>\n      <td>Apple can not be drunk</td>\n      <td>Apple cannot eat a human</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jeff ran 100,000 miles today</td>\n      <td>100,000 miles is way to long for one person to...</td>\n      <td>Jeff is a four letter name and 100,000 has six...</td>\n      <td>100,000 miles is longer than 100,000 km.</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I sting a mosquito</td>\n      <td>A human is a mammal</td>\n      <td>A human is omnivorous</td>\n      <td>A human has not stings</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A giraffe is a person.</td>\n      <td>Giraffes can drink water from a lake.</td>\n      <td>A giraffe is not a human being.</td>\n      <td>.Giraffes usually eat leaves.</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>Mark ate a big bitter cherry pie</td>\n      <td>Mark is bad at making cherry pie</td>\n      <td>a cherry pie should be big</td>\n      <td>a cherry pie should be sweet</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>Gloria wears a cat on her head</td>\n      <td>a hat cannot be worn on a cat's head</td>\n      <td>a cat cannot be worn on a person's head</td>\n      <td>the cat is too heavy to be worn on her head</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>Harry went to the barbershop to have his glass...</td>\n      <td>a barbershop usually don't provide the service...</td>\n      <td>a barbershop usually repairs computers instead...</td>\n      <td>the barbershop lacked the necessary tools to r...</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>Reilly is sleeping on the window</td>\n      <td>the window is open and a person cannot lay on it</td>\n      <td>the window is too cold to sleep on it</td>\n      <td>a person cannot sleep on a window</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>I have a desk on my lamp</td>\n      <td>the lamp is made of glass that is fragile</td>\n      <td>the lamp is of poor quality</td>\n      <td>a desk is too big to be put on a lamp</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2Model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2Model.from_pretrained('gpt2')\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model.eval()\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# def score(sentence):\n",
    "#     tokenize_input = tokenizer.tokenize(sentence)\n",
    "#     tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "#     loss=model(tensor_input, labels=tensor_input)\n",
    "#     return -loss[0] * len(tokenize_input)\n",
    "\n",
    "# a=['there is a book on the desk',\n",
    "#                 'there is a plane on the desk',\n",
    "#                         'there is a book in the desk']\n",
    "# print([score(i) for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "# model_id = 'gpt2'\n",
    "# # model_id = 'gpt2-large'\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "source": [
    "## New section\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n# Load pre-trained model tokenizer (vocabulary)\\ntokenizer = BertTokenizer.from_pretrained('./tmp/swag_output/')\\n# Load pre-trained model (weights)\\nmodel = BertForMaskedLM.from_pretrained('./tmp/swag_output/')\\nmodel.eval()\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "def predict(text, bert_model, bert_tokenizer):\n",
    "    # Tokenized input\n",
    "    # text = \"[CLS] I got restricted because Tom reported my reply [SEP]\"\n",
    "    text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = bert_tokenizer.tokenize(text)\n",
    "    # text = \"[CLS] Stir the mixture until it is done [SEP]\"\n",
    "        #masked_index = 4\n",
    "    sentence_prob = 1\n",
    "    for masked_index in range(1,len(tokenized_text)-1):\n",
    "        # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "        masked_word = tokenized_text[masked_index]\n",
    "        #tokenized_text[masked_index] = '[MASK]'\n",
    "        # assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "        # print (tokenized_text)\n",
    "\n",
    "        # Convert token to vocabulary indices\n",
    "        indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "        # segments_ids = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "        length = len(tokenized_text)\n",
    "        segments_ids = [0 for _ in range(length)]\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        # If you have a GPU, put everything on cuda\n",
    "        # tokens_tensor = tokens_tensor.to('cuda')\n",
    "        # segments_tensors = segments_tensors.to('cuda')\n",
    "\n",
    "        # Load pre-trained model (weights)\n",
    "        # bert_model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "        # bert_model.eval()\n",
    "\n",
    "        # If you have a GPU, put everything on cuda\n",
    "        # tokens_tensor = tokens_tensor.to('cuda')\n",
    "        # segments_tensors = segments_tensors.to('cuda')\n",
    "        # bert_model.to('cuda')\n",
    "\n",
    "        # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            predictions = bert_model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        predictions = torch.nn.functional.softmax(predictions, -1)\n",
    "\n",
    "        index = bert_tokenizer.convert_tokens_to_ids([masked_word])[0]\n",
    "\n",
    "        curr_prob = predictions[0, masked_index][index]\n",
    "        # predict_list = predictions[0, masked_index]\n",
    "        sentence_prob *= curr_prob\n",
    "        #tokenized_text[masked_index] = masked_word\n",
    "    return math.pow(sentence_prob, 1/(len(tokenized_text)-3))\n",
    "    #return sentence_prob\n",
    "'''\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('./tmp/swag_output/')\n",
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('./tmp/swag_output/')\n",
    "model.eval()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0<00:31, 1544198.58B/s]\u001b[A\n",
      " 88%|████████▊ | 359649280/407873900 [09:30<00:31, 1553055.35B/s]\u001b[A\n",
      " 88%|████████▊ | 359822336/407873900 [09:30<00:31, 1542313.59B/s]\u001b[A\n",
      " 88%|████████▊ | 359997440/407873900 [09:30<00:30, 1563077.97B/s]\u001b[A\n",
      " 88%|████████▊ | 360155136/407873900 [09:30<00:30, 1539702.15B/s]\u001b[A\n",
      " 88%|████████▊ | 360345600/407873900 [09:30<00:30, 1564692.46B/s]\u001b[A\n",
      " 88%|████████▊ | 360503296/407873900 [09:30<00:30, 1539458.94B/s]\u001b[A\n",
      " 88%|████████▊ | 360693760/407873900 [09:31<00:29, 1593171.41B/s]\u001b[A\n",
      " 88%|████████▊ | 360867840/407873900 [09:31<00:29, 1615617.75B/s]\u001b[A\n",
      " 89%|████████▊ | 361058304/407873900 [09:31<00:28, 1616441.01B/s]\u001b[A\n",
      " 89%|████████▊ | 361244672/407873900 [09:31<00:27, 1683341.95B/s]\u001b[A\n",
      " 89%|████████▊ | 361423872/407873900 [09:31<00:27, 1698299.26B/s]\u001b[A\n",
      " 89%|████████▊ | 361598976/407873900 [09:31<00:28, 1644340.55B/s]\u001b[A\n",
      " 89%|████████▊ | 361790464/407873900 [09:31<00:27, 1679420.24B/s]\u001b[A\n",
      " 89%|████████▊ | 361981952/407873900 [09:31<00:26, 1710925.97B/s]\u001b[A\n",
      " 89%|████████▉ | 362173440/407873900 [09:31<00:27, 1691464.56B/s]\u001b[A\n",
      " 89%|████████▉ | 362364928/407873900 [09:32<00:26, 1718014.05B/s]\u001b[A\n",
      " 89%|████████▉ | 362556416/407873900 [09:32<00:26, 1703989.67B/s]\u001b[A\n",
      " 89%|████████▉ | 362747904/407873900 [09:32<00:30, 1497403.91B/s]\u001b[A\n",
      " 89%|████████▉ | 362939392/407873900 [09:32<00:31, 1424403.26B/s]\u001b[A\n",
      " 89%|████████▉ | 363130880/407873900 [09:32<00:29, 1542134.51B/s]\u001b[A\n",
      " 89%|████████▉ | 363291648/407873900 [09:32<00:34, 1281695.39B/s]\u001b[A\n",
      " 89%|████████▉ | 363431936/407873900 [09:32<00:36, 1228372.25B/s]\u001b[A\n",
      " 89%|████████▉ | 363564032/407873900 [09:33<00:40, 1102145.39B/s]\u001b[A\n",
      " 89%|████████▉ | 363682816/407873900 [09:33<00:41, 1055153.99B/s]\u001b[A\n",
      " 89%|████████▉ | 363808768/407873900 [09:33<00:42, 1042540.35B/s]\u001b[A\n",
      " 89%|████████▉ | 363918336/407873900 [09:33<00:44, 992548.52B/s] \u001b[A\n",
      " 89%|████████▉ | 364036096/407873900 [09:33<00:42, 1036234.16B/s]\u001b[A\n",
      " 89%|████████▉ | 364143616/407873900 [09:33<00:41, 1041934.39B/s]\u001b[A\n",
      " 89%|████████▉ | 364262400/407873900 [09:33<00:42, 1022079.11B/s]\u001b[A\n",
      " 89%|████████▉ | 364366848/407873900 [09:33<00:42, 1025521.61B/s]\u001b[A\n",
      " 89%|████████▉ | 364488704/407873900 [09:33<00:42, 1017674.73B/s]\u001b[A\n",
      " 89%|████████▉ | 364601344/407873900 [09:34<00:41, 1047981.15B/s]\u001b[A\n",
      " 89%|████████▉ | 364732416/407873900 [09:34<00:40, 1054331.18B/s]\u001b[A\n",
      " 89%|████████▉ | 364838912/407873900 [09:34<00:40, 1053817.57B/s]\u001b[A\n",
      " 89%|████████▉ | 364976128/407873900 [09:34<00:40, 1068247.11B/s]\u001b[A\n",
      " 90%|████████▉ | 365115392/407873900 [09:34<00:38, 1100233.57B/s]\u001b[A\n",
      " 90%|████████▉ | 365237248/407873900 [09:34<00:39, 1076809.34B/s]\u001b[A\n",
      " 90%|████████▉ | 365376512/407873900 [09:34<00:37, 1142755.85B/s]\u001b[A\n",
      " 90%|████████▉ | 365533184/407873900 [09:34<00:36, 1158601.23B/s]\u001b[A\n",
      " 90%|████████▉ | 365672448/407873900 [09:34<00:34, 1215459.04B/s]\u001b[A\n",
      " 90%|████████▉ | 365811712/407873900 [09:35<00:36, 1150712.24B/s]\u001b[A\n",
      " 90%|████████▉ | 365985792/407873900 [09:35<00:35, 1186057.15B/s]\u001b[A\n",
      " 90%|████████▉ | 366142464/407873900 [09:35<00:32, 1271664.89B/s]\u001b[A\n",
      " 90%|████████▉ | 366272512/407873900 [09:35<00:34, 1203310.94B/s]\u001b[A\n",
      " 90%|████████▉ | 366438400/407873900 [09:35<00:31, 1306350.09B/s]\u001b[A\n",
      " 90%|████████▉ | 366573568/407873900 [09:35<00:31, 1296654.85B/s]\u001b[A\n",
      " 90%|████████▉ | 366716928/407873900 [09:35<00:31, 1290467.16B/s]\u001b[A\n",
      " 90%|████████▉ | 366891008/407873900 [09:35<00:30, 1348981.75B/s]\u001b[A\n",
      " 90%|████████▉ | 367029248/407873900 [09:36<00:30, 1328660.47B/s]\u001b[A\n",
      " 90%|█████████ | 367186944/407873900 [09:36<00:30, 1339467.72B/s]\u001b[A\n",
      " 90%|█████████ | 367378432/407873900 [09:36<00:28, 1403809.94B/s]\u001b[A\n",
      " 90%|█████████ | 367535104/407873900 [09:36<00:28, 1400517.37B/s]\u001b[A\n",
      " 90%|█████████ | 367676416/407873900 [09:36<00:29, 1360571.29B/s]\u001b[A\n",
      " 90%|█████████ | 367848448/407873900 [09:36<00:28, 1390549.02B/s]\u001b[A\n",
      " 90%|█████████ | 368005120/407873900 [09:36<00:28, 1418313.98B/s]\u001b[A\n",
      " 90%|█████████ | 368179200/407873900 [09:36<00:27, 1433408.67B/s]\u001b[A\n",
      " 90%|█████████ | 368323584/407873900 [09:36<00:27, 1412790.50B/s]\u001b[A\n",
      " 90%|█████████ | 368492544/407873900 [09:37<00:27, 1419959.65B/s]\u001b[A\n",
      " 90%|█████████ | 368685056/407873900 [09:37<00:25, 1541244.12B/s]\u001b[A\n",
      " 90%|█████████ | 368842752/407873900 [09:37<00:26, 1449725.49B/s]\u001b[A\n",
      " 90%|█████████ | 369032192/407873900 [09:37<00:25, 1552940.58B/s]\u001b[A\n",
      " 91%|█████████ | 369191936/407873900 [09:37<00:24, 1547998.05B/s]\u001b[A\n",
      " 91%|█████████ | 369380352/407873900 [09:37<00:24, 1558529.11B/s]\u001b[A\n",
      " 91%|█████████ | 369571840/407873900 [09:37<00:23, 1630693.92B/s]\u001b[A\n",
      " 91%|█████████ | 369737728/407873900 [09:37<00:23, 1592241.17B/s]\u001b[A\n",
      " 91%|█████████ | 369972224/407873900 [09:37<00:23, 1616050.27B/s]\u001b[A\n",
      " 91%|█████████ | 370181120/407873900 [09:38<00:21, 1719451.08B/s]\u001b[A\n",
      " 91%|█████████ | 370372608/407873900 [09:38<00:21, 1741766.08B/s]\u001b[A\n",
      " 91%|█████████ | 370549760/407873900 [09:38<00:22, 1654238.65B/s]\u001b[A\n",
      " 91%|█████████ | 370790400/407873900 [09:38<00:22, 1675256.44B/s]\u001b[A\n",
      " 91%|█████████ | 371034112/407873900 [09:38<00:21, 1695196.66B/s]\u001b[A\n",
      " 91%|█████████ | 371295232/407873900 [09:38<00:20, 1773662.40B/s]\u001b[A\n",
      " 91%|█████████ | 371538944/407873900 [09:38<00:19, 1900282.76B/s]\u001b[A\n",
      " 91%|█████████ | 371733504/407873900 [09:38<00:19, 1900784.46B/s]\u001b[A\n",
      " 91%|█████████ | 371927040/407873900 [09:39<00:19, 1817988.89B/s]\u001b[A\n",
      " 91%|█████████ | 372113408/407873900 [09:39<00:19, 1801602.27B/s]\u001b[A\n",
      " 91%|█████████▏| 372339712/407873900 [09:39<00:19, 1864273.18B/s]\u001b[A\n",
      " 91%|█████████▏| 372528128/407873900 [09:39<00:19, 1856311.49B/s]\u001b[A\n",
      " 91%|█████████▏| 372773888/407873900 [09:39<00:18, 1924187.84B/s]\u001b[A\n",
      " 91%|█████████▏| 372968448/407873900 [09:39<00:18, 1913589.87B/s]\u001b[A\n",
      " 91%|█████████▏| 373175296/407873900 [09:39<00:18, 1895568.23B/s]\u001b[A\n",
      " 92%|█████████▏| 373419008/407873900 [09:39<00:17, 1995500.32B/s]\u001b[A\n",
      " 92%|█████████▏| 373620736/407873900 [09:39<00:17, 1982489.41B/s]\u001b[A\n",
      " 92%|█████████▏| 373854208/407873900 [09:39<00:17, 1954425.92B/s]\u001b[A\n",
      " 92%|█████████▏| 374063104/407873900 [09:40<00:17, 1977630.85B/s]\u001b[A\n",
      " 92%|█████████▏| 374289408/407873900 [09:40<00:16, 2019773.03B/s]\u001b[A\n",
      " 92%|█████████▏| 374532096/407873900 [09:40<00:16, 2068685.60B/s]\u001b[A\n",
      " 92%|█████████▏| 374742016/407873900 [09:40<00:16, 2044236.82B/s]\u001b[A\n",
      " 92%|█████████▏| 374968320/407873900 [09:40<00:15, 2096818.68B/s]\u001b[A\n",
      " 92%|█████████▏| 375194624/407873900 [09:40<00:15, 2046982.09B/s]\u001b[A\n",
      " 92%|█████████▏| 375490560/407873900 [09:40<00:14, 2218620.69B/s]\u001b[A\n",
      " 92%|█████████▏| 375717888/407873900 [09:40<00:17, 1831872.90B/s]\u001b[A\n",
      " 92%|█████████▏| 375995392/407873900 [09:41<00:15, 1995376.74B/s]\u001b[A\n",
      " 92%|█████████▏| 376209408/407873900 [09:41<00:18, 1722169.56B/s]\u001b[A\n",
      " 92%|█████████▏| 376398848/407873900 [09:41<00:19, 1625998.95B/s]\u001b[A\n",
      " 92%|█████████▏| 376574976/407873900 [09:41<00:25, 1213317.15B/s]\u001b[A\n",
      " 92%|█████████▏| 376721408/407873900 [09:41<00:31, 999253.70B/s] \u001b[A\n",
      " 92%|█████████▏| 376845312/407873900 [09:41<00:38, 804121.38B/s]\u001b[A\n",
      " 92%|█████████▏| 376948736/407873900 [09:42<00:40, 766118.60B/s]\u001b[A\n",
      " 92%|█████████▏| 377041920/407873900 [09:42<00:43, 714560.46B/s]\u001b[A\n",
      " 92%|█████████▏| 377125888/407873900 [09:42<00:44, 683731.20B/s]\u001b[A\n",
      " 92%|█████████▏| 377212928/407873900 [09:42<00:47, 650466.46B/s]\u001b[A\n",
      " 93%|█████████▎| 377300992/407873900 [09:42<00:48, 631807.85B/s]\u001b[A\n",
      " 93%|█████████▎| 377405440/407873900 [09:42<00:46, 651330.67B/s]\u001b[A\n",
      " 93%|█████████▎| 377509888/407873900 [09:43<00:45, 665587.78B/s]\u001b[A\n",
      " 93%|█████████▎| 377631744/407873900 [09:43<00:42, 703835.90B/s]\u001b[A\n",
      " 93%|█████████▎| 377736192/407873900 [09:43<00:42, 707011.61B/s]\u001b[A\n",
      " 93%|█████████▎| 377858048/407873900 [09:43<00:40, 750110.90B/s]\u001b[A\n",
      " 93%|█████████▎| 377945088/407873900 [09:43<00:38, 772993.81B/s]\u001b[A\n",
      " 93%|█████████▎| 378032128/407873900 [09:43<00:39, 759637.13B/s]\u001b[A\n",
      " 93%|█████████▎| 378136576/407873900 [09:43<00:36, 820552.32B/s]\u001b[A\n",
      " 93%|█████████▎| 378221568/407873900 [09:43<00:37, 789033.19B/s]\u001b[A\n",
      " 93%|█████████▎| 378328064/407873900 [09:44<00:35, 840321.27B/s]\u001b[A\n",
      " 93%|█████████▎| 378449920/407873900 [09:44<00:35, 833561.63B/s]\u001b[A\n",
      " 93%|█████████▎| 378589184/407873900 [09:44<00:34, 847219.99B/s]\u001b[A\n",
      " 93%|█████████▎| 378711040/407873900 [09:44<00:34, 856537.81B/s]\u001b[A\n",
      " 93%|█████████▎| 378850304/407873900 [09:44<00:31, 907111.08B/s]\u001b[A\n",
      " 93%|█████████▎| 378989568/407873900 [09:44<00:30, 936137.49B/s]\u001b[A\n",
      " 93%|█████████▎| 379146240/407873900 [09:44<00:28, 998221.48B/s]\u001b[A\n",
      " 93%|█████████▎| 379268096/407873900 [09:44<00:27, 1023580.14B/s]\u001b[A\n",
      " 93%|█████████▎| 379407360/407873900 [09:45<00:28, 999232.97B/s] \u001b[A\n",
      " 93%|█████████▎| 379564032/407873900 [09:45<00:27, 1032144.23B/s]\u001b[A\n",
      " 93%|█████████▎| 379720704/407873900 [09:45<00:27, 1039075.21B/s]\u001b[A\n",
      " 93%|█████████▎| 379877376/407873900 [09:45<00:26, 1041498.61B/s]\u001b[A\n",
      " 93%|█████████▎| 380034048/407873900 [09:45<00:26, 1044769.09B/s]\u001b[A\n",
      " 93%|█████████▎| 380207104/407873900 [09:45<00:25, 1078100.77B/s]\u001b[A\n",
      " 93%|█████████▎| 380364800/407873900 [09:45<00:25, 1090656.95B/s]\u001b[A\n",
      " 93%|█████████▎| 380538880/407873900 [09:46<00:23, 1149570.25B/s]\u001b[A\n",
      " 93%|█████████▎| 380712960/407873900 [09:46<00:22, 1181692.77B/s]\u001b[A\n",
      " 93%|█████████▎| 380887040/407873900 [09:46<00:22, 1206306.85B/s]\u001b[A\n",
      " 93%|█████████▎| 381043712/407873900 [09:46<00:20, 1289615.65B/s]\u001b[A\n",
      " 93%|█████████▎| 381200384/407873900 [09:46<00:21, 1250333.35B/s]\u001b[A\n",
      " 94%|█████████▎| 381391872/407873900 [09:46<00:21, 1260340.32B/s]\u001b[A\n",
      " 94%|█████████▎| 381565952/407873900 [09:46<00:20, 1262957.20B/s]\u001b[A\n",
      " 94%|█████████▎| 381757440/407873900 [09:47<00:20, 1297613.85B/s]\u001b[A\n",
      " 94%|█████████▎| 381948928/407873900 [09:47<00:19, 1338121.04B/s]\u001b[A\n",
      " 94%|█████████▎| 382140416/407873900 [09:47<00:18, 1374730.06B/s]\u001b[A\n",
      " 94%|█████████▎| 382349312/407873900 [09:47<00:17, 1421926.35B/s]\u001b[A\n",
      " 94%|█████████▍| 382540800/407873900 [09:47<00:17, 1433913.43B/s]\u001b[A\n",
      " 94%|█████████▍| 382732288/407873900 [09:47<00:16, 1480294.10B/s]\u001b[A\n",
      " 94%|█████████▍| 382941184/407873900 [09:47<00:16, 1468000.36B/s]\u001b[A\n",
      " 94%|█████████▍| 383150080/407873900 [09:47<00:16, 1495962.55B/s]\u001b[A\n",
      " 94%|█████████▍| 383341568/407873900 [09:48<00:15, 1549089.91B/s]\u001b[A\n",
      " 94%|█████████▍| 383533056/407873900 [09:48<00:15, 1582072.16B/s]\u001b[A\n",
      " 94%|█████████▍| 383692800/407873900 [09:48<00:15, 1545807.20B/s]\u001b[A\n",
      " 94%|█████████▍| 383862784/407873900 [09:48<00:15, 1523886.58B/s]\u001b[A\n",
      " 94%|█████████▍| 384016384/407873900 [09:48<00:15, 1492217.50B/s]\u001b[A\n",
      " 94%|█████████▍| 384194560/407873900 [09:48<00:15, 1510294.91B/s]\u001b[A\n",
      " 94%|█████████▍| 384346112/407873900 [09:48<00:15, 1510280.36B/s]\u001b[A\n",
      " 94%|█████████▍| 384542720/407873900 [09:48<00:15, 1537167.16B/s]\u001b[A\n",
      " 94%|█████████▍| 384734208/407873900 [09:49<00:14, 1551320.63B/s]\u001b[A\n",
      " 94%|█████████▍| 384960512/407873900 [09:49<00:14, 1564872.89B/s]\u001b[A\n",
      " 94%|█████████▍| 385186816/407873900 [09:49<00:14, 1529274.41B/s]\u001b[A\n",
      " 94%|█████████▍| 385430528/407873900 [09:49<00:13, 1655396.75B/s]\u001b[A\n",
      " 95%|█████████▍| 385622016/407873900 [09:49<00:14, 1561675.26B/s]\u001b[A\n",
      " 95%|█████████▍| 385813504/407873900 [09:49<00:14, 1515983.92B/s]\u001b[A\n",
      " 95%|█████████▍| 386022400/407873900 [09:49<00:14, 1536744.17B/s]\u001b[A\n",
      " 95%|█████████▍| 386231296/407873900 [09:49<00:13, 1551905.69B/s]\u001b[A\n",
      " 95%|█████████▍| 386440192/407873900 [09:50<00:13, 1552339.55B/s]\u001b[A\n",
      " 95%|█████████▍| 386666496/407873900 [09:50<00:13, 1601033.25B/s]\u001b[A\n",
      " 95%|█████████▍| 386892800/407873900 [09:50<00:12, 1622213.08B/s]\u001b[A\n",
      " 95%|█████████▍| 387119104/407873900 [09:50<00:12, 1633869.87B/s]\u001b[A\n",
      " 95%|█████████▍| 387328000/407873900 [09:50<00:11, 1737108.24B/s]\u001b[A\n",
      " 95%|█████████▌| 387519488/407873900 [09:50<00:11, 1765871.01B/s]\u001b[A\n",
      " 95%|█████████▌| 387698688/407873900 [09:50<00:12, 1679944.91B/s]\u001b[A\n",
      " 95%|█████████▌| 387937280/407873900 [09:50<00:11, 1708417.39B/s]\u001b[A\n",
      " 95%|█████████▌| 388111360/407873900 [09:51<00:13, 1467113.85B/s]\u001b[A\n",
      " 95%|█████████▌| 388302848/407873900 [09:51<00:12, 1566138.20B/s]\u001b[A\n",
      " 95%|█████████▌| 388466688/407873900 [09:51<00:15, 1292334.86B/s]\u001b[A\n",
      " 95%|█████████▌| 388609024/407873900 [09:51<00:17, 1102740.23B/s]\u001b[A\n",
      " 95%|█████████▌| 388733952/407873900 [09:51<00:17, 1102452.22B/s]\u001b[A\n",
      " 95%|█████████▌| 388854784/407873900 [09:51<00:19, 998369.33B/s] \u001b[A\n",
      " 95%|█████████▌| 388963328/407873900 [09:51<00:19, 988621.31B/s]\u001b[A\n",
      " 95%|█████████▌| 389068800/407873900 [09:52<00:21, 890631.99B/s]\u001b[A\n",
      " 95%|█████████▌| 389189632/407873900 [09:52<00:19, 945121.38B/s]\u001b[A\n",
      " 95%|█████████▌| 389289984/407873900 [09:52<00:19, 948610.28B/s]\u001b[A\n",
      " 95%|█████████▌| 389389312/407873900 [09:52<00:19, 950803.08B/s]\u001b[A\n",
      " 95%|█████████▌| 389487616/407873900 [09:52<00:20, 911045.65B/s]\u001b[A\n",
      " 96%|█████████▌| 389591040/407873900 [09:52<00:19, 931042.64B/s]\u001b[A\n",
      " 96%|█████████▌| 389695488/407873900 [09:52<00:19, 933294.53B/s]\u001b[A\n",
      " 96%|█████████▌| 389816320/407873900 [09:52<00:18, 990746.04B/s]\u001b[A\n",
      " 96%|█████████▌| 389917696/407873900 [09:52<00:18, 996387.86B/s]\u001b[A\n",
      " 96%|█████████▌| 390026240/407873900 [09:53<00:18, 957657.30B/s]\u001b[A\n",
      " 96%|█████████▌| 390148096/407873900 [09:53<00:18, 975822.42B/s]\u001b[A\n",
      " 96%|█████████▌| 390269952/407873900 [09:53<00:17, 1025858.11B/s]\u001b[A\n",
      " 96%|█████████▌| 390374400/407873900 [09:53<00:17, 977986.68B/s] \u001b[A\n",
      " 96%|█████████▌| 390496256/407873900 [09:53<00:16, 1029244.78B/s]\u001b[A\n",
      " 96%|█████████▌| 390601728/407873900 [09:53<00:16, 1034081.98B/s]\u001b[A\n",
      " 96%|█████████▌| 390739968/407873900 [09:53<00:16, 1062400.46B/s]\u001b[A\n",
      " 96%|█████████▌| 390847488/407873900 [09:53<00:16, 1060270.54B/s]\u001b[A\n",
      " 96%|█████████▌| 391001088/407873900 [09:53<00:15, 1116647.48B/s]\u001b[A\n",
      " 96%|█████████▌| 391140352/407873900 [09:54<00:14, 1146050.72B/s]\u001b[A\n",
      " 96%|█████████▌| 391280640/407873900 [09:54<00:13, 1212583.89B/s]\u001b[A\n",
      " 96%|█████████▌| 391404544/407873900 [09:54<00:14, 1139729.17B/s]\u001b[A\n",
      " 96%|█████████▌| 391558144/407873900 [09:54<00:13, 1232373.81B/s]\u001b[A\n",
      " 96%|█████████▌| 391685120/407873900 [09:54<00:13, 1165421.28B/s]\u001b[A\n",
      " 96%|█████████▌| 391854080/407873900 [09:54<00:13, 1198241.40B/s]\u001b[A\n",
      " 96%|█████████▌| 392028160/407873900 [09:54<00:12, 1260629.85B/s]\u001b[A\n",
      " 96%|█████████▌| 392184832/407873900 [09:54<00:12, 1287584.03B/s]\u001b[A\n",
      " 96%|█████████▌| 392358912/407873900 [09:55<00:11, 1339781.62B/s]\u001b[A\n",
      " 96%|█████████▌| 392495104/407873900 [09:55<00:11, 1319215.58B/s]\u001b[A\n",
      " 96%|█████████▋| 392654848/407873900 [09:55<00:11, 1334330.86B/s]\u001b[A\n",
      " 96%|█████████▋| 392811520/407873900 [09:55<00:11, 1361430.18B/s]\u001b[A\n",
      " 96%|█████████▋| 392950784/407873900 [09:55<00:11, 1321907.09B/s]\u001b[A\n",
      " 96%|█████████▋| 393107456/407873900 [09:55<00:10, 1352445.24B/s]\u001b[A\n",
      " 96%|█████████▋| 393246720/407873900 [09:55<00:11, 1311337.56B/s]\u001b[A\n",
      " 96%|█████████▋| 393420800/407873900 [09:55<00:10, 1384915.51B/s]\u001b[A\n",
      " 96%|█████████▋| 393561088/407873900 [09:55<00:10, 1336508.27B/s]\u001b[A\n",
      " 97%|█████████▋| 393734144/407873900 [09:56<00:10, 1401278.64B/s]\u001b[A\n",
      " 97%|█████████▋| 393876480/407873900 [09:56<00:10, 1354289.00B/s]\u001b[A\n",
      " 97%|█████████▋| 394064896/407873900 [09:56<00:09, 1446897.84B/s]\u001b[A\n",
      " 97%|█████████▋| 394213376/407873900 [09:56<00:09, 1401676.64B/s]\u001b[A\n",
      " 97%|█████████▋| 394394624/407873900 [09:56<00:09, 1464293.13B/s]\u001b[A\n",
      " 97%|█████████▋| 394587136/407873900 [09:56<00:08, 1502171.60B/s]\u001b[A\n",
      " 97%|█████████▋| 394739712/407873900 [09:56<00:08, 1498373.75B/s]\u001b[A\n",
      " 97%|█████████▋| 394917888/407873900 [09:56<00:08, 1522848.86B/s]\u001b[A\n",
      " 97%|█████████▋| 395109376/407873900 [09:56<00:08, 1579872.41B/s]\u001b[A\n",
      " 97%|█████████▋| 395269120/407873900 [09:57<00:08, 1550002.01B/s]\u001b[A\n",
      " 97%|█████████▋| 395457536/407873900 [09:57<00:07, 1580672.63B/s]\u001b[A\n",
      " 97%|█████████▋| 395666432/407873900 [09:57<00:07, 1618611.55B/s]\u001b[A\n",
      " 97%|█████████▋| 395840512/407873900 [09:57<00:07, 1637802.63B/s]\u001b[A\n",
      " 97%|█████████▋| 396032000/407873900 [09:57<00:07, 1625487.95B/s]\u001b[A\n",
      " 97%|█████████▋| 396275712/407873900 [09:57<00:06, 1664951.96B/s]\u001b[A\n",
      " 97%|█████████▋| 396484608/407873900 [09:57<00:06, 1736225.50B/s]\u001b[A\n",
      " 97%|█████████▋| 396694528/407873900 [09:57<00:06, 1831168.01B/s]\u001b[A\n",
      " 97%|█████████▋| 396879872/407873900 [09:57<00:06, 1749827.31B/s]\u001b[A\n",
      " 97%|█████████▋| 397086720/407873900 [09:58<00:05, 1834586.30B/s]\u001b[A\n",
      " 97%|█████████▋| 397285376/407873900 [09:58<00:05, 1793566.86B/s]\u001b[A\n",
      " 97%|█████████▋| 397467648/407873900 [09:58<00:05, 1765915.21B/s]\u001b[A\n",
      " 98%|█████████▊| 397720576/407873900 [09:58<00:05, 1806538.91B/s]\u001b[A\n",
      " 98%|█████████▊| 397964288/407873900 [09:58<00:05, 1909444.81B/s]\u001b[A\n",
      " 98%|█████████▊| 398157824/407873900 [09:58<00:05, 1832184.28B/s]\u001b[A\n",
      " 98%|█████████▊| 398364672/407873900 [09:58<00:05, 1864276.26B/s]\u001b[A\n",
      " 98%|█████████▊| 398608384/407873900 [09:58<00:04, 1874906.38B/s]\u001b[A\n",
      " 98%|█████████▊| 398853120/407873900 [09:58<00:04, 2016271.01B/s]\u001b[A\n",
      " 98%|█████████▊| 399058944/407873900 [09:59<00:04, 2009751.03B/s]\u001b[A\n",
      " 98%|█████████▊| 399262720/407873900 [09:59<00:04, 2005197.43B/s]\u001b[A\n",
      " 98%|█████████▊| 399495168/407873900 [09:59<00:04, 2006572.01B/s]\u001b[A\n",
      " 98%|█████████▊| 399739904/407873900 [09:59<00:03, 2072049.33B/s]\u001b[A\n",
      " 98%|█████████▊| 400017408/407873900 [09:59<00:03, 2183571.78B/s]\u001b[A\n",
      " 98%|█████████▊| 400244736/407873900 [09:59<00:03, 2122498.52B/s]\u001b[A\n",
      " 98%|█████████▊| 400539648/407873900 [09:59<00:03, 2270617.36B/s]\u001b[A\n",
      " 98%|█████████▊| 400801792/407873900 [09:59<00:02, 2363111.26B/s]\u001b[A\n",
      " 98%|█████████▊| 401080320/407873900 [09:59<00:02, 2425889.65B/s]\u001b[A\n",
      " 98%|█████████▊| 401358848/407873900 [10:00<00:02, 2454733.84B/s]\u001b[A\n",
      " 98%|█████████▊| 401672192/407873900 [10:00<00:02, 2552026.98B/s]\u001b[A\n",
      " 99%|█████████▊| 401968128/407873900 [10:00<00:02, 2633032.97B/s]\u001b[A\n",
      " 99%|█████████▊| 402264064/407873900 [10:00<00:02, 2716490.76B/s]\u001b[A\n",
      " 99%|█████████▊| 402594816/407873900 [10:00<00:01, 2843082.15B/s]\u001b[A\n",
      " 99%|█████████▉| 402925568/407873900 [10:00<00:01, 2947782.67B/s]\u001b[A\n",
      " 99%|█████████▉| 403224576/407873900 [10:00<00:01, 2913802.64B/s]\u001b[A\n",
      " 99%|█████████▉| 403518464/407873900 [10:00<00:01, 2633135.54B/s]\u001b[A\n",
      " 99%|█████████▉| 403788800/407873900 [10:00<00:01, 2487385.40B/s]\u001b[A\n",
      " 99%|█████████▉| 404057088/407873900 [10:01<00:01, 2469527.47B/s]\u001b[A\n",
      " 99%|█████████▉| 404353024/407873900 [10:01<00:01, 2476526.12B/s]\u001b[A\n",
      " 99%|█████████▉| 404631552/407873900 [10:01<00:01, 2561298.71B/s]\u001b[A\n",
      " 99%|█████████▉| 404927488/407873900 [10:01<00:01, 2486548.72B/s]\u001b[A\n",
      " 99%|█████████▉| 405223424/407873900 [10:01<00:01, 2603295.68B/s]\u001b[A\n",
      " 99%|█████████▉| 405538816/407873900 [10:01<00:00, 2747036.79B/s]\u001b[A\n",
      " 99%|█████████▉| 405818368/407873900 [10:01<00:00, 2622047.14B/s]\u001b[A\n",
      "100%|█████████▉| 406111232/407873900 [10:01<00:00, 2675537.86B/s]\u001b[A\n",
      "100%|█████████▉| 406407168/407873900 [10:01<00:00, 2750541.72B/s]\u001b[A\n",
      "100%|█████████▉| 406737920/407873900 [10:02<00:00, 2702587.85B/s]\u001b[A\n",
      "100%|█████████▉| 407090176/407873900 [10:02<00:00, 2905457.17B/s]\u001b[A\n",
      "100%|█████████▉| 407387136/407873900 [10:02<00:00, 2862072.49B/s]\u001b[A\n",
      "100%|██████████| 407873900/407873900 [10:02<00:00, 677086.44B/s] \n",
      "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpxkypv7uu to cache at /home/yashar/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /home/yashar/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpxkypv7uu\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/yashar/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /home/yashar/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmptvc_nvu_\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "# 'bert-base-uncased'\n",
    "# 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "# prob = predict(sentence_1, bert_model=model, bert_tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "B\n",
      "1\n",
      "1 0\n",
      "A\n",
      "1 1\n",
      "B\n",
      "1 2\n",
      "B\n",
      "1 3\n",
      "A\n",
      "1 4\n",
      "B\n",
      "1 5\n",
      "C\n",
      "2\n",
      "2 6\n",
      "B\n",
      "3\n",
      "3 7\n",
      "A\n",
      "3 8\n",
      "A\n",
      "3 9\n",
      "C\n",
      "4\n",
      "4 10\n",
      "A\n",
      "4 11\n",
      "B\n",
      "4 12\n",
      "C\n",
      "4 13\n",
      "B\n",
      "4 14\n",
      "A\n",
      "5\n",
      "5 15\n",
      "A\n",
      "5 16\n",
      "B\n",
      "6\n",
      "6 17\n",
      "C\n",
      "7\n",
      "7 18\n",
      "B\n",
      "7 19\n",
      "A\n",
      "7 20\n",
      "B\n",
      "7 21\n",
      "B\n",
      "8\n",
      "8 22\n",
      "B\n",
      "8 23\n",
      "A\n",
      "9\n",
      "9 24\n",
      "C\n",
      "9 25\n",
      "A\n",
      "10\n",
      "10 26\n",
      "A\n",
      "10 27\n",
      "C\n",
      "10 28\n",
      "C\n",
      "10 29\n",
      "B\n",
      "10 30\n",
      "C\n",
      "10 31\n",
      "B\n",
      "10 32\n",
      "A\n",
      "11\n",
      "11 33\n",
      "C\n",
      "12\n",
      "12 34\n",
      "A\n",
      "12 35\n",
      "B\n",
      "12 36\n",
      "C\n",
      "13\n",
      "13 37\n",
      "B\n",
      "13 38\n",
      "B\n",
      "14\n",
      "14 39\n",
      "B\n",
      "14 40\n",
      "B\n",
      "15\n",
      "15 41\n",
      "B\n",
      "15 42\n",
      "A\n",
      "15 43\n",
      "A\n",
      "15 44\n",
      "A\n",
      "15 45\n",
      "B\n",
      "16\n",
      "16 46\n",
      "C\n",
      "16 47\n",
      "C\n",
      "16 48\n",
      "A\n",
      "17\n",
      "17 49\n",
      "B\n",
      "17 50\n",
      "A\n",
      "17 51\n",
      "A\n",
      "18\n",
      "18 52\n",
      "C\n",
      "18 53\n",
      "C\n",
      "18 54\n",
      "B\n",
      "18 55\n",
      "A\n",
      "19\n",
      "19 56\n",
      "B\n",
      "19 57\n",
      "C\n",
      "19 58\n",
      "B\n",
      "20\n",
      "20 59\n",
      "B\n",
      "20 60\n",
      "A\n",
      "20 61\n",
      "A\n",
      "21\n",
      "21 62\n",
      "A\n",
      "21 63\n",
      "A\n",
      "21 64\n",
      "A\n",
      "21 65\n",
      "C\n",
      "21 66\n",
      "C\n",
      "21 67\n",
      "C\n",
      "22\n",
      "22 68\n",
      "B\n",
      "22 69\n",
      "B\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-5c6f1c3b12d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprob_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprob_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprob_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-817244748f65>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(text, bert_model, bert_tokenizer)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Predict all tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, masked_lm_labels)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n\u001b[0m\u001b[1;32m    758\u001b[0m                                        output_all_encoded_layers=False)\n\u001b[1;32m    759\u001b[0m         \u001b[0mprediction_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         encoded_layers = self.encoder(embedding_output,\n\u001b[0m\u001b[1;32m    628\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                                       output_all_encoded_layers=output_all_encoded_layers)\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBertEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"ConcatenatedSentences.txt\", \"r\") as f:\n",
    "    file = f.readlines()\n",
    "\n",
    "num = len(file)\n",
    "count = 0\n",
    "curr = 0\n",
    "for i in file:\n",
    "    label, sentence_1, sentence_2, sentence_3 = i.split(\"\\001\")\n",
    "    print (label[0])\n",
    "    prob_1 = predict(sentence_1, bert_model=model, bert_tokenizer=tokenizer)\n",
    "    prob_2 = predict(sentence_2, bert_model=model, bert_tokenizer=tokenizer)\n",
    "    prob_3 = predict(sentence_3, bert_model=model, bert_tokenizer=tokenizer)\n",
    "    answer = max(prob_1, prob_2, prob_3)\n",
    "    if (prob_1 == answer and label[0] == \"A\") or (prob_2 == answer and label[0] == \"B\") \\\n",
    "            or (prob_3 == answer and label[0] == \"C\"):\n",
    "        count += 1\n",
    "        print (count)\n",
    "\n",
    "    print(count, curr)\n",
    "    curr += 1\n",
    "print (count/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('nlp_env': conda)",
   "display_name": "Python 3.8.5 64-bit ('nlp_env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d3b11afc1419d2e372e9e23e645a6813b53ad6225361ec94a05870210063ecc9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}